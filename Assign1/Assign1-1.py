# -*- coding: utf-8 -*-
# @Author: Haiqin Yang
# @Date:   2025-10-05 18:20:05
# @Last Modified by:   Haiqin Yang
# @Last Modified time: 2025-10-07 15:37:54

#!/usr/bin/env python3

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

from nltk.stem import PorterStemmer

from nltk.corpus import wordnet, stopwords

from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()

from nltk.probability import FreqDist
 
import requests
from urllib.parse import urlparse

import re
from bs4 import BeautifulSoup

import contractions

import unicodedata

from opencc import OpenCC
import jieba
cc_zh = OpenCC('t2s')  # ä¸­æ–‡ç¹ä½“è½¬ç®€ä½“

from tqdm import tqdm  # å¯¼å…¥tqdmåº“
import argparse
import json
from pathlib import Path  # ç”¨äºè·¯å¾„éªŒè¯

def detect_language(text):
    '''
    æ›´ä¸¥è°¨çš„å®ç°æ˜¯ä½¿ç”¨langdetectåŒ…ï¼Œä½†è¯¥åŒ…æœ‰æ—¶ä¼šè¯¯åˆ¤ä¸­è‹±æ··åˆæ–‡æœ¬ä¸ºå…¶ä»–è¯­è¨€
    éœ€è¦å®‰è£…ï¼špip install langdetect
    from langdetect import detect, LangDetectException

    def detect_language(text):
        if not text.strip():
            return 'unknown'
        try:
            lang = detect(text)
            # æ˜ å°„ä¸ºä¸­æ–‡ï¼ˆzhï¼‰æˆ–è‹±æ–‡ï¼ˆenï¼‰ï¼Œå…¶ä»–è¯­è¨€è¿”å›unknown
            return 'zh' if lang == 'zh-cn' else 'en' if lang == 'en' else 'unknown'
        except LangDetectException:
            return 'unknown'
    '''
    if not text.strip():
        return 'unknown'
    
    # æå–ä¸­æ–‡CJKå­—ç¬¦å’Œè‹±æ–‡å•è¯å­—ç¬¦
    cjk_chars = re.findall(r'[\u4e00-\u9fff]', text)
    en_chars = re.findall(r'[a-zA-Z]', text)  # è‹±æ–‡ç‰¹å¾ï¼šå­—æ¯
    
    total = len(text.strip())
    if total == 0:
        return 'unknown'
    
    cjk_ratio = len(cjk_chars) / total
    en_ratio = len(en_chars) / total
    
    # ä¸­æ–‡åˆ¤å®šï¼šCJKå æ¯”è¶…30%
    if cjk_ratio > 0.3:
        return 'zh'
    # è‹±æ–‡åˆ¤å®šï¼šè‹±æ–‡å æ¯”è¶…30%ï¼Œä¸”CJKå æ¯”æä½ï¼ˆé¿å…ä¸­è‹±æ··åˆæ–‡æœ¬è¯¯åˆ¤ï¼‰
    elif en_ratio > 0.3 and cjk_ratio < 0.1:
        return 'en'
    # å…¶ä»–æƒ…å†µï¼ˆå¦‚ä¸­è‹±æ··åˆã€å…¶ä»–è¯­è¨€ï¼‰
    else:
        return 'unknown'

def get_statistics(text, n=10, k=10, lang='en'):
    '''
    è·å–æ–‡æœ¬ä¸­å‰nä¸ªé«˜é¢‘è¯å’Œé•¿åº¦æœ€é•¿çš„kä¸ªè¯
    
    å‚æ•°ï¼š
        text: è¾“å…¥æ–‡æœ¬
        n: é«˜é¢‘è¯æ•°é‡ï¼ˆé»˜è®¤10ï¼‰
        k: æœ€é•¿è¯æ•°é‡ï¼ˆé»˜è®¤10ï¼‰
        lang: è¯­è¨€ï¼ˆ'en'è‹±æ–‡/'zh'ä¸­æ–‡ï¼Œé»˜è®¤'en'ï¼‰
    
    è¿”å›ï¼š
        å…ƒç»„ (top_n_words, longest_k_words)ï¼Œå…¶ä¸­ï¼š
            - top_n_words: åˆ—è¡¨ï¼Œå…ƒç´ ä¸º (è¯, é¢‘ç‡) å…ƒç»„ï¼ŒæŒ‰é¢‘ç‡é™åºæ’åˆ—
            - longest_k_words: åˆ—è¡¨ï¼Œå…ƒç´ ä¸º (è¯, é•¿åº¦) å…ƒç»„ï¼ŒæŒ‰é•¿åº¦é™åºæ’åˆ—ï¼ˆé•¿åº¦ç›¸åŒåˆ™æŒ‰è¯æœ¬èº«æ’åºï¼‰
    '''    
    if not text.strip():
        return ([], [])  # ç©ºæ–‡æœ¬è¿”å›ä¸¤ä¸ªç©ºåˆ—è¡¨
    
    # 1. åˆ†è¯ï¼ˆä¸­è‹±æ–‡é€‚é…ï¼‰
    if lang == 'en':
        tokens = nltk.word_tokenize(text)
    elif lang == 'zh':
        tokens = jieba.lcut(text)  # ä¸­æ–‡ç²¾ç¡®åˆ†è¯
    else:
        raise ValueError("Unsupported language. Use 'en' or 'zh'.")
    
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼ˆç§»é™¤çº¯ç©ºæ ¼/ç©ºçš„tokenï¼‰
    tokens = [token.strip() for token in tokens]
    tokens = [token for token in tokens if token]  # ä¿ç•™éç©ºtoken
    
    # 2. è®¡ç®—å‰nä¸ªé«˜é¢‘è¯ï¼ˆå¤ç”¨åŸé€»è¾‘ï¼‰
    fdist = FreqDist(tokens)
    top_n_words = fdist.most_common(n)  # æ ¼å¼ï¼š[(è¯1, é¢‘ç‡1), (è¯2, é¢‘ç‡2), ...]
    
    # 3. è®¡ç®—é•¿åº¦æœ€é•¿çš„kä¸ªè¯ï¼ˆå»é‡åæŒ‰é•¿åº¦æ’åºï¼‰
    unique_tokens = list(set(tokens))  # å»é‡ï¼Œé¿å…é‡å¤è¯å æ®ä½ç½®
    # æŒ‰é•¿åº¦é™åºæ’åºï¼ˆé•¿åº¦ç›¸åŒåˆ™æŒ‰è¯çš„å­—æ¯/å­—ç¬¦é¡ºåºæ’åºï¼Œä¿è¯ç¨³å®šæ€§ï¼‰
    sorted_by_length = sorted(
        unique_tokens,
        key=lambda x: (-len(x), x)  # å…ˆæŒ‰é•¿åº¦çš„è´Ÿæ•°ï¼ˆé™åºï¼‰ï¼Œå†æŒ‰è¯æœ¬èº«ï¼ˆå‡åºï¼‰
    )
    # å–å‰kä¸ªï¼Œå¹¶æ·»åŠ é•¿åº¦ä¿¡æ¯
    longest_k_words = [(word, len(word)) for word in sorted_by_length[:k]]
    
    return (top_n_words, longest_k_words)

def strip_html_tags(text):
    """
    ç§»é™¤æ–‡æœ¬ä¸­çš„HTMLæ ‡ç­¾å¹¶æå–çº¯æ–‡æœ¬å†…å®¹ï¼ŒåŒæ—¶æ ‡å‡†åŒ–æ¢è¡Œç¬¦ã€‚

    åŠŸèƒ½è¯´æ˜ï¼š
        1. ä½¿ç”¨BeautifulSoupè§£æHTMLæ–‡æœ¬ï¼Œç§»é™¤`<iframe>`å’Œ`<script>`æ ‡ç­¾ï¼ˆé€šå¸¸åŒ…å«éæ–‡æœ¬å†…å®¹ï¼‰
        2. æå–HTMLä¸­çš„çº¯æ–‡æœ¬å†…å®¹
        3. å°†å„ç§æ¢è¡Œç¬¦ï¼ˆ\rã€\nã€\r\nï¼‰ç»Ÿä¸€æ›¿æ¢ä¸ºå•ä¸ª\nï¼Œç¡®ä¿æ¢è¡Œæ ¼å¼ä¸€è‡´

    å‚æ•°ï¼š
        text (str)ï¼šåŒ…å«HTMLæ ‡ç­¾çš„åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²

    è¿”å›ï¼š
        strï¼šç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾åçš„çº¯æ–‡æœ¬ï¼Œæ¢è¡Œç¬¦å·²æ ‡å‡†åŒ–ä¸º\n

    ä¾èµ–ï¼š
        éœ€è¦å®‰è£…BeautifulSoupåº“ï¼ˆpip install beautifulsoup4ï¼‰

    ç¤ºä¾‹ï¼š
        >>> raw_html = "<p>Hello<br>World!</p><script>alert('test')</script>"
        >>> strip_html_tags(raw_html)
        'Hello\nWorld!'
    """
    pass
    
def remove_accented_chars(text):
    """
    ç§»é™¤æ–‡æœ¬ä¸­çš„é‡éŸ³å­—ç¬¦ï¼ˆå¦‚Ã©ã€Ã±ã€Ã¼ç­‰ï¼‰ï¼Œå°†å…¶è½¬æ¢ä¸ºæ— é‡éŸ³çš„åŸºç¡€å­—ç¬¦ã€‚
    
    å¤„ç†é€»è¾‘ï¼š
        1. ä½¿ç”¨`unicodedata.normalize('NFKD', text)`å¯¹æ–‡æœ¬è¿›è¡ŒUnicodeè§„èŒƒåŒ–ï¼š
           - NFKDï¼ˆCompatibility Decomposition, Canonical Compositionï¼‰ä¼šå°†é‡éŸ³å­—ç¬¦åˆ†è§£ä¸º
             åŸºç¡€å­—ç¬¦ + é‡éŸ³ç¬¦å·ï¼ˆä¾‹å¦‚ï¼š'Ã©' åˆ†è§£ä¸º 'e' + é‡éŸ³ç¬¦å·ï¼‰ã€‚
        2. é€šè¿‡`.encode('ascii', 'ignore')`å°†è§„èŒƒåŒ–åçš„æ–‡æœ¬ç¼–ç ä¸ºASCIIï¼š
           - ASCIIç¼–ç ä¸æ”¯æŒé‡éŸ³ç¬¦å·ï¼Œ`ignore`å‚æ•°ä¼šå¿½ç•¥æ— æ³•ç¼–ç çš„é‡éŸ³ç¬¦å·éƒ¨åˆ†ï¼Œä»…ä¿ç•™åŸºç¡€å­—ç¬¦ã€‚
        3. å†é€šè¿‡`.decode('utf-8', 'ignore')`å°†å­—èŠ‚æµè§£ç å›UTF-8å­—ç¬¦ä¸²ï¼Œå¾—åˆ°æ— é‡éŸ³çš„ç»“æœã€‚
    
    å‚æ•°ï¼š
        text (str)ï¼šåŒ…å«é‡éŸ³å­—ç¬¦çš„åŸå§‹æ–‡æœ¬
    
    è¿”å›ï¼š
        strï¼šç§»é™¤é‡éŸ³åçš„æ–‡æœ¬ï¼ˆä»…åŒ…å«ASCIIå¯è¡¨ç¤ºçš„å­—ç¬¦ï¼‰
    
    ç¤ºä¾‹ï¼š
        >>> remove_accented_chars("cafÃ© clichÃ© naÃ¯ve")
        'cafe cliche naive'
        >>> remove_accented_chars("Ã Ã¨Ã¬Ã²Ã¹ ÃÃ‰ÃÃ“Ãš Ã± Ã§")
        'aeiou AEIOU n c'
    """
    pass
    
def pos_tag_wordnet(tagged_tokens):
    """
    å°†NLTKè¯æ€§æ ‡æ³¨ç»“æœè½¬æ¢ä¸ºWordNetè¯å½¢è¿˜åŸå™¨ï¼ˆLemmatizerï¼‰å…¼å®¹çš„è¯æ€§æ ‡ç­¾ã€‚
    
    èƒŒæ™¯ï¼š
        NLTKçš„`pos_tag`å‡½æ•°è¿”å›çš„è¯æ€§æ ‡ç­¾éµå¾ªPenn Treebankæ ¼å¼ï¼ˆå¦‚å½¢å®¹è¯ä¸º'JJ'ã€åŠ¨è¯ä¸º'VB'ç­‰ï¼‰ï¼Œ
        è€ŒWordNetçš„`lemmatize`æ–¹æ³•éœ€è¦ç‰¹å®šçš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚å½¢å®¹è¯ä¸º`wordnet.ADJ`ã€åŠ¨è¯ä¸º`wordnet.VERB`ç­‰ï¼‰ï¼Œ
        å› æ­¤éœ€è¦é€šè¿‡é¦–å­—æ¯æ˜ å°„å®ç°æ ¼å¼è½¬æ¢ã€‚
    
    å‚æ•°ï¼š
        tagged_tokens (list)ï¼šç”±`nltk.pos_tag`è¿”å›çš„è¯æ€§æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„`(word, tag)`ï¼Œ
                             å…¶ä¸­`word`æ˜¯è¯è¯­ï¼Œ`tag`æ˜¯Penn Treebankæ ¼å¼çš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚'JJ'ã€'VB'ï¼‰ã€‚
    
    è¿”å›ï¼š
        listï¼šè½¬æ¢åçš„è¯æ€§æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„`(word, wordnet_tag)`ï¼Œ
              å…¶ä¸­`wordnet_tag`æ˜¯WordNetå…¼å®¹çš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚`wordnet.ADJ`ã€`wordnet.VERB`ï¼‰ã€‚
              è‹¥æ ‡ç­¾æ— æ³•åŒ¹é…ï¼Œé»˜è®¤æ˜ å°„ä¸ºåè¯ï¼ˆ`wordnet.NOUN`ï¼‰ã€‚
    
    æ˜ å°„è§„åˆ™ï¼š
        - 'j'ï¼ˆå½¢å®¹è¯ï¼Œå¦‚Pennæ ‡ç­¾'JJ'ï¼‰â†’ `wordnet.ADJ`
        - 'v'ï¼ˆåŠ¨è¯ï¼Œå¦‚Pennæ ‡ç­¾'VB'ï¼‰â†’ `wordnet.VERB`
        - 'n'ï¼ˆåè¯ï¼Œå¦‚Pennæ ‡ç­¾'NN'ï¼‰â†’ `wordnet.NOUN`
        - 'r'ï¼ˆå‰¯è¯ï¼Œå¦‚Pennæ ‡ç­¾'RB'ï¼‰â†’ `wordnet.ADV`
    
    ç¤ºä¾‹ï¼š
        >>> from nltk import pos_tag, word_tokenize
        >>> tagged = pos_tag(word_tokenize("running fast"))  # [('running', 'VBG'), ('fast', 'RB')]
        >>> pos_tag_wordnet(tagged)
        [('running', wordnet.VERB), ('fast', wordnet.ADV)]
    """
    pass


def lemmatize_text(text):
    """
    å¯¹æ–‡æœ¬è¿›è¡Œè¯å½¢è¿˜åŸï¼ˆLemmatizationï¼‰ï¼Œå°†è¯è¯­è¿˜åŸä¸ºå…¶åŸºæœ¬å½¢å¼ï¼ˆå¦‚åŠ¨è¯ç¬¬ä¸‰äººç§°â†’åŸå½¢ã€åè¯å¤æ•°â†’å•æ•°ï¼‰ã€‚
    
    å¤„ç†æµç¨‹ï¼š
        1. åˆ†è¯ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºç‹¬ç«‹è¯è¯­ï¼ˆä½¿ç”¨NLTKçš„`word_tokenize`ï¼‰ã€‚
        2. è¯æ€§æ ‡æ³¨ï¼šä¸ºæ¯ä¸ªè¯è¯­æ·»åŠ Penn Treebankæ ¼å¼çš„è¯æ€§æ ‡ç­¾ï¼ˆä½¿ç”¨NLTKçš„`pos_tag`ï¼‰ã€‚
        3. æ ‡ç­¾è½¬æ¢ï¼šå°†è¯æ€§æ ‡ç­¾è½¬æ¢ä¸ºWordNetå…¼å®¹æ ¼å¼ï¼ˆè°ƒç”¨`pos_tag_wordnet`ï¼‰ã€‚
        4. è¯å½¢è¿˜åŸï¼šä½¿ç”¨WordNetè¯å½¢è¿˜åŸå™¨ï¼Œæ ¹æ®è½¬æ¢åçš„è¯æ€§æ ‡ç­¾å¯¹æ¯ä¸ªè¯è¯­è¿›è¡Œè¿˜åŸã€‚
        5. æ‹¼æ¥ï¼šå°†è¿˜åŸåçš„è¯è¯­é‡æ–°æ‹¼æ¥ä¸ºæ–‡æœ¬ã€‚
    
    å‚æ•°ï¼š
        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ï¼ˆè‹±æ–‡ï¼‰ã€‚
    
    è¿”å›ï¼š
        strï¼šç»è¿‡è¯å½¢è¿˜åŸåçš„æ–‡æœ¬ï¼Œè¯è¯­å‡ä¸ºåŸºæœ¬å½¢å¼ã€‚
    
    ä¾èµ–ï¼š
        - éœ€è¦åŠ è½½NLTKçš„`punkt`åˆ†è¯æ¨¡å‹å’Œ`averaged_perceptron_tagger`è¯æ€§æ ‡æ³¨æ¨¡å‹ï¼ˆå¯é€šè¿‡`nltk.download()`ä¸‹è½½ï¼‰ã€‚
        - éœ€è¦åˆå§‹åŒ–WordNetè¯å½¢è¿˜åŸå™¨ï¼ˆå¦‚`wnl = WordNetLemmatizer()`ï¼‰ã€‚
    
    ç¤ºä¾‹ï¼š
        >>> wnl = WordNetLemmatizer()  # å‡è®¾å·²åˆå§‹åŒ–
        >>> lemmatize_text("Cats are running quickly")
        'cat be run quick'
    """
    pass       

def remove_special_characters(text, lang='en', remove_digits=False):
    """
    ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ã€ç¬¦å·ç­‰ï¼‰ï¼Œå¯é€‰æ‹©æ˜¯å¦ä¿ç•™æ•°å­—ã€‚
    
    å¤„ç†é€»è¾‘ï¼š
        1. æ ¹æ®`remove_digits`å‚æ•°æ„å»ºæ­£åˆ™åŒ¹é…æ¨¡å¼:
           - å½“`remove_digits=False`ï¼ˆé»˜è®¤ï¼‰:
             - lang='en': ä¿ç•™å­—æ¯(a-zA-Z)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚
             - lang='zh': ä¿ç•™ä¸­æ–‡(ä¸€-é¾¥)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚
           - å½“`remove_digits=True`:
             - lang='en': ä¿ç•™å­—æ¯(a-zA-Z)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚
             - lang='zh': ä¿ç•™ä¸­æ–‡(ä¸€-é¾¥)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚
        2. ä½¿ç”¨`re.sub`å°†åŒ¹é…åˆ°çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå®ç°ç§»é™¤æ•ˆæœã€‚
        3. é¢å¤–å¤„ç†ï¼šç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦ï¼Œç¡®ä¿æ–‡æœ¬æ•´æ´ã€‚
    
    å‚æ•°ï¼š
        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ã€‚
        remove_digits (bool)ï¼šæ˜¯å¦ç§»é™¤æ•°å­—ï¼Œé»˜è®¤Falseï¼ˆä¿ç•™æ•°å­—ï¼‰ã€‚
        remove_special_characters (bool): æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤Falseï¼ˆä¿ç•™ç‰¹æ®Šå­—ç¬¦ï¼‰
    
    è¿”å›ï¼š
        strï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦åçš„æ–‡æœ¬ï¼ˆä»…ä¿ç•™æŒ‡å®šå…è®¸çš„å­—ç¬¦ï¼‰ã€‚
    
    ç¤ºä¾‹ï¼š
        >>> remove_special_characters("Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚", lang='en')
        'Hello  123'
        
        >>> remove_special_characters("Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚", lang='zh')
        'ä¸–ç•Œ 123'
        
        >>> remove_special_characters("Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚", lang='en', remove_digits=True)
        'Hello '
        
        >>> remove_special_characters("Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚", lang='zh', remove_digits=True)
        'ä¸–ç•Œ'    
    """
    pass

def remove_stopwords(text, is_lower_case=False, stopwords=None, lang='en'):
    """
    ç§»é™¤æ–‡æœ¬ä¸­çš„åœç”¨è¯ï¼ˆå¦‚è‹±æ–‡çš„"the"ã€"is"ï¼Œä¸­æ–‡çš„"çš„"ã€"äº†"ç­‰æ— å®é™…è¯­ä¹‰çš„é«˜é¢‘è¯ï¼‰ï¼Œæ”¯æŒä¸­è‹±æ–‡ã€‚
    
    å¤„ç†æµç¨‹ï¼š
        1. åŠ è½½åœç”¨è¯è¡¨ï¼šè‹¥æœªæä¾›è‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼ˆstopwordsï¼‰ï¼Œåˆ™æ ¹æ®è¯­è¨€ï¼ˆlangï¼‰åŠ è½½é»˜è®¤åœç”¨è¯è¡¨ï¼š
           - è‹±æ–‡ï¼šä½¿ç”¨NLTKçš„è‹±æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.corpus.stopwords.words('english')ï¼‰ã€‚
           - ä¸­æ–‡ï¼šä½¿ç”¨NLTKçš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.corpus.stopwords.words('chinese')ï¼‰ã€‚
        2. åˆ†è¯ï¼šæ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯å·¥å…·ï¼š
           - è‹±æ–‡ï¼šä½¿ç”¨NLTKçš„`word_tokenize`è¿›è¡Œåˆ†è¯ã€‚
           - ä¸­æ–‡ï¼šä½¿ç”¨Jiebaçš„`lcut`ï¼ˆç²¾ç¡®æ¨¡å¼ï¼‰è¿›è¡Œåˆ†è¯ã€‚
        3. è¿‡æ»¤åœç”¨è¯ï¼šæ ¹æ®`is_lower_case`åˆ¤æ–­æ˜¯å¦éœ€è¦å°†è¯è¯­å°å†™åå†åŒ¹é…åœç”¨è¯è¡¨ï¼Œä¿ç•™éåœç”¨è¯ã€‚
        4. æ‹¼æ¥ï¼šå°†è¿‡æ»¤åçš„è¯è¯­é‡æ–°æ‹¼æ¥ä¸ºæ–‡æœ¬ã€‚
    
    å‚æ•°ï¼š
        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ã€‚
        is_lower_case (bool)ï¼šæ–‡æœ¬æ˜¯å¦å·²è½¬ä¸ºå°å†™ï¼Œé»˜è®¤Falseï¼ˆéœ€å°†è¯è¯­å°å†™åå†åŒ¹é…åœç”¨è¯ï¼‰ã€‚
        stopwords (list, optional)ï¼šè‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼Œè‹¥ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è¡¨ã€‚
        lang (str)ï¼šè¯­è¨€ç±»å‹ï¼Œ'en'ï¼ˆè‹±æ–‡ï¼‰æˆ–'zh'ï¼ˆä¸­æ–‡ï¼‰ï¼Œé»˜è®¤'en'ã€‚
    
    è¿”å›ï¼š
        strï¼šåˆ‡è¯å¹¶ç§»é™¤åœç”¨è¯åç”¨ç©ºæ ¼ç›¸è¿çš„æ–‡æœ¬ã€‚
    
    ä¾èµ–ï¼š
        - è‹±æ–‡ï¼šéœ€åŠ è½½NLTKçš„`punkt`åˆ†è¯æ¨¡å‹ï¼ˆnltk.download('punkt')ï¼‰å’Œè‹±æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.download('stopwords')ï¼‰ã€‚
        - ä¸­æ–‡ï¼šéœ€å®‰è£…Jiebaï¼ˆpip install jiebaï¼‰å’ŒåŠ è½½NLTKçš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.download('stopwords')ï¼‰ã€‚
    
    ç¤ºä¾‹ï¼š
        >>> # è‹±æ–‡ç¤ºä¾‹
        >>> remove_stopwords("The quick brown fox jumps over the lazy dog", lang='en')
        'quick brown fox jumps lazy dog'
        >>> # ä¸­æ–‡ç¤ºä¾‹
        >>> remove_stopwords("è¿™åªæ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†é‚£åªæ‡’ç‹—", lang='zh')
        'åª æ•æ· æ£•è‰² ç‹ç‹¸ è·³è¿‡ åª æ‡’ ç‹—'
    """
    pass    
 
def normalize_doc(doc, 
                     html_stripping=True, 
                     contraction_expansion=True,
                     accented_char_removal=True, 
                     text_lower_case=True,
                     text_lemmatization=True, 
                     special_char_removal=True,
                     stopword_removal=True, 
                     remove_digits=False, # Default: keep digits
                     zh_simplification=True,
                     isDebug=False):
    """
    è§„èŒƒåŒ–æ–‡æœ¬è¯­æ–™åº“ï¼Œæ”¯æŒå¤šç§é¢„å¤„ç†æ“ä½œï¼ŒåŒ…æ‹¬HTMLæ ‡ç­¾ç§»é™¤ã€ç¼©å†™æ‰©å±•ã€é‡éŸ³å­—ç¬¦ç§»é™¤ã€
    å°å†™è½¬æ¢ã€è¯å½¢è¿˜åŸã€ç‰¹æ®Šå­—ç¬¦ç§»é™¤ã€åœç”¨è¯ç§»é™¤ç­‰ï¼Œé€‚ç”¨äºä¸­è‹±æ–‡æ–‡æœ¬ã€‚
    å‚æ•°ï¼š
        doc (str): è¾“å…¥æ–‡æ¡£ã€‚
        html_stripping (bool): æ˜¯å¦ç§»é™¤HTMLæ ‡ç­¾ï¼Œé»˜è®¤Trueã€‚
        contraction_expansion (bool): æ˜¯å¦æ‰©å±•ç¼©å†™ï¼Œé»˜è®¤Trueã€‚
        accented_char_removal (bool): æ˜¯å¦ç§»é™¤é‡éŸ³å­—ç¬¦ï¼Œé»˜è®¤Trueã€‚
        text_lower_case (bool): æ˜¯å¦å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œé»˜è®¤Trueã€‚
        text_lemmatization (bool): æ˜¯å¦è¿›è¡Œè¯å½¢è¿˜åŸï¼Œé»˜è®¤Trueã€‚
        special_char_removal (bool): æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤Trueã€‚
        stopword_removal (bool): æ˜¯å¦ç§»é™¤åœç”¨è¯ï¼Œé»˜è®¤Trueã€‚
        remove_digits (bool): æ˜¯å¦ç§»é™¤æ•°å­—ï¼Œé»˜è®¤Falseã€‚
        zh_simplification (bool): æ˜¯å¦å°†ä¸­æ–‡ç¹ä½“è½¬æ¢ä¸ºç®€ä½“ï¼Œé»˜è®¤Trueã€‚
        isDebug (bool): æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œé»˜è®¤Falseã€‚
    è¿”å›ï¼š
        å…ƒç»„ `(doc, lang)`ï¼Œå…¶ä¸­ï¼š
            - doc (str)ï¼šé¢„å¤„ç†åçš„æ–‡æœ¬(åˆ‡è¯åç”¨ç©ºæ ¼ç›¸è¿)
            - lang (str)ï¼šæ–‡æœ¬è¯­è¨€ï¼ˆ'en'/'zh'/'unknown'ï¼‰    
    """                              
    pass


def fetch_gutenberg_text(file_path=None):
    """
    è¯»å–æœ¬åœ°å¤è…¾å ¡txtæ–‡ä»¶ï¼Œæå–æ­£æ–‡å†…å®¹ï¼ˆå»é™¤é¦–å°¾å…ƒæ•°æ®æ ‡è®°ï¼‰
    
    å‚æ•°ï¼š
        file_path (str): æœ¬åœ°txtæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ './data/1342-0.txt'
    
    è¿”å›ï¼š
        str: æå–çš„æ­£æ–‡å†…å®¹ï¼›è‹¥æ–‡ä»¶è¯»å–å¤±è´¥/æ— æœ‰æ•ˆæ ‡è®°ï¼Œè¿”å›None
    
    ç¤ºä¾‹ï¼š
        >>> content = fetch_gutenberg_text('./data/1342-0.txt')
        >>> print(content[:500])  # æ‰“å°æ­£æ–‡å‰500å­—ç¬¦
           
    è¯´æ˜: 
        ä½ å¯ä»¥è‡ªå·±å®ç°è¾“å…¥ fetch_gutenberg_text(link='https://www.gutenberg.org/files/1342/1342-0.txt'):
        >>> content = fetch_gutenberg_text('https://www.gutenberg.org/files/1342/1342-0.txt')
        >>> print(content[:500])  # æ‰“å°æ­£æ–‡å‰500å­—ç¬¦
    """
    # 1. éªŒè¯æ–‡ä»¶æ ¼å¼ï¼ˆä»…æ”¯æŒtxtï¼‰
    if not file_path.endswith('.txt'):
        print(f"é”™è¯¯ï¼š{file_path} ä¸æ˜¯.txtæ ¼å¼æ–‡ä»¶ï¼Œä»…æ”¯æŒæ–‡æœ¬æ–‡ä»¶")
        return None

    # 2. æ­£åˆ™åŒ¹é…å¤è…¾å ¡æ ‡å‡†æ ‡è®°ï¼ˆæå–æ­£æ–‡æ ¸å¿ƒï¼‰
    # åŒ¹é… "*** START OF THE PROJECT GUTENBERG EBOOK ... ***" åŠå˜ä½“
    start_pat = re.compile(
        r'\*{3}\s+START OF (?:THE|THIS)?\s+PROJECT GUTENBERG EBOOK.*?\*{3}',
        re.IGNORECASE | re.DOTALL
    )
    # åŒ¹é… "*** END OF THE PROJECT GUTENBERG EBOOK ... ***" åŠå˜ä½“
    end_pat = re.compile(
        r'\*{3}\s+END OF (?:THE|THIS)?\s+PROJECT GUTENBERG EBOOK.*?\*{3}',
        re.IGNORECASE | re.DOTALL
    )

    # 3. è¯»å–æœ¬åœ°æ–‡ä»¶ï¼ˆé€‚é…å¤è…¾å ¡å¸¸è§ç¼–ç ï¼šutf-8 / latin-1ï¼‰
    try:
        print(f"æ­£åœ¨è¯»å–æœ¬åœ°æ–‡ä»¶ï¼š{file_path}")
        
        # ä¼˜å…ˆå°è¯•utf-8ç¼–ç ï¼ˆå¤è…¾å ¡ç°ä»£æ–‡æœ¬å¸¸ç”¨ï¼‰
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            print("æ–‡ä»¶ç¼–ç ï¼šutf-8ï¼ˆè¯»å–æˆåŠŸï¼‰")
        
        # è‹¥utf-8è§£ç å¤±è´¥ï¼Œå°è¯•latin-1ï¼ˆå¤è…¾å ¡æ—©æœŸè‹±æ–‡æ–‡æœ¬å¸¸ç”¨ï¼‰
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='latin-1') as f:
                text = f.read()
            print("æ–‡ä»¶ç¼–ç ï¼šlatin-1ï¼ˆè¯»å–æˆåŠŸï¼‰")

    # æ•è·æœ¬åœ°æ–‡ä»¶è¯»å–çš„å¸¸è§é”™è¯¯
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return None
    except PermissionError:
        print(f"é”™è¯¯ï¼šæ— æƒé™è¯»å–æ–‡ä»¶ {file_path}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™")
        return None
    except Exception as e:
        print(f"é”™è¯¯ï¼šè¯»å–æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ - {str(e)[:50]}...")
        return None

    # 4. æå–æ­£æ–‡ï¼ˆåŸºäºå¤è…¾å ¡æ ‡è®°ï¼‰
    start_match = start_pat.search(text)
    end_match = end_pat.search(text)

    # å¤„ç†æ ‡è®°ä¸å­˜åœ¨/ä½ç½®å¼‚å¸¸çš„æƒ…å†µ
    if not start_match:
        print("è­¦å‘Šï¼šæœªæ‰¾åˆ°å¤è…¾å ¡æ­£æ–‡å¼€å§‹æ ‡è®°ï¼ˆ*** START OF ... ***ï¼‰")
        # å¯é€‰ï¼šè¿”å›å…¨æ–‡ï¼ˆè‹¥ç”¨æˆ·å¸Œæœ›ä¿ç•™å®Œæ•´æ–‡ä»¶å†…å®¹ï¼‰
        # return text.strip()
        return None
    if not end_match:
        print("è­¦å‘Šï¼šæœªæ‰¾åˆ°å¤è…¾å ¡æ­£æ–‡ç»“æŸæ ‡è®°ï¼ˆ*** END OF ... ***ï¼‰")
        # å¯é€‰ï¼šè¿”å›å¼€å§‹æ ‡è®°åçš„å†…å®¹
        # return text[start_match.end():].strip()
        return None
    if start_match.end() >= end_match.start():
        print("é”™è¯¯ï¼šå¼€å§‹æ ‡è®°ä½ç½®åœ¨ç»“æŸæ ‡è®°ä¹‹åï¼Œæ— æ³•æå–æ­£æ–‡")
        return None

    # æå–æ ‡è®°é—´çš„æ­£æ–‡ï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼‰
    content = text[start_match.end():end_match.start()].strip()
    print(f"æ­£æ–‡æå–æˆåŠŸï¼æ­£æ–‡é•¿åº¦ï¼š{len(content)} å­—ç¬¦")
    return content

from pathlib import Path  # ç¡®ä¿å·²å¯¼å…¥

def save_processed_text(original_path, processed_text):
    """
    å°†é¢„å¤„ç†åçš„æ–‡æœ¬ä¿å­˜è‡³æ–°æ–‡ä»¶ï¼Œå‘½åè§„åˆ™ï¼šåŸæ–‡ä»¶xxx.txt â†’ xxx-p.txt
    
    å‚æ•°ï¼š
        original_path (str): åŸæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ './data/1342-0.txt'ï¼‰
        processed_text (str): é¢„å¤„ç†åçš„æ–‡æœ¬å†…å®¹
    """
    # è½¬æ¢ä¸ºPathå¯¹è±¡ï¼Œæ–¹ä¾¿å¤„ç†è·¯å¾„
    orig_path = Path(original_path)
    
    # ç”Ÿæˆæ–°æ–‡ä»¶åï¼šåŸæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰+ "-p" + æ‰©å±•å
    new_filename = f"{orig_path.stem}-p{orig_path.suffix}"  # å¦‚ "1342-0-p.txt"
    
    # æ„å»ºæ–°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼ˆä¸åŸæ–‡ä»¶åŒç›®å½•ï¼‰
    new_path = orig_path.parent / new_filename  # å¦‚ './data/1342-0-p.txt'
    
    try:
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
        new_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å†™å…¥é¢„å¤„ç†åçš„æ–‡æœ¬ï¼ˆä½¿ç”¨utf-8ç¼–ç ï¼Œé¿å…ä¸­æ–‡ä¹±ç ï¼‰
        with open(new_path, 'w', encoding='utf-8') as f:
            f.write(processed_text)
        
        print(f"âœ… é¢„å¤„ç†æ–‡æœ¬å·²ä¿å­˜è‡³ï¼š{new_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ {new_path}ï¼š{str(e)}")

def test_english_contractions(processed: str)->list:
    """
    æ£€æŸ¥å¤„ç†åçš„æ–‡æœ¬ä¸­æ˜¯å¦ä»åŒ…å«æœªæ‰©å±•çš„è‹±æ–‡ç¼©å†™æ ¼å¼
    
    å‚æ•°ï¼š
        processed: å¤„ç†åçš„æ–‡æœ¬
    è¿”å›ï¼š
        é”™è¯¯åˆ—è¡¨ï¼ˆåŒ…å«ä»å­˜åœ¨çš„ç¼©å†™æ ¼å¼ï¼‰
    """
    errors = []
    # ç»Ÿä¸€è½¬ä¸ºå°å†™æ£€æŸ¥ï¼ˆå¿½ç•¥å¤§å°å†™å½±å“ï¼‰
    processed_lower = processed.lower()
    
    # éå†contractionsåŒ…æ”¯æŒçš„æ‰€æœ‰ç¼©å†™
    for contraction in contractions.contractions_dict.keys():
        # 1. æ£€æŸ¥å¤„ç†åçš„æ–‡æœ¬ä¸­æ˜¯å¦å­˜åœ¨è¯¥ç¼©å†™ï¼ˆå®Œæ•´å•è¯åŒ¹é…ï¼‰
        # æ­£åˆ™ï¼š ç¡®ä¿åŒ¹é…ç‹¬ç«‹å•è¯ï¼Œre.escapeå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æ’‡å·'ï¼‰
        contraction_pattern = re.compile(rf'{re.escape(contraction)}')
        if contraction_pattern.search(processed_lower):
            # è‹¥å­˜åœ¨æœªæ‰©å±•çš„ç¼©å†™ï¼Œè®°å½•é”™è¯¯
            errors.append(f"å¤„ç†åçš„æ–‡æœ¬ä»åŒ…å«æœªæ‰©å±•çš„ç¼©å†™: '{contraction}'")
    
    return errors

def test_stopwords(processed:str, lang='en')->list:
    """
    å‡è®¾processedæ˜¯å·²ç»åˆ‡è¯ç”¨ç©ºæ ¼åˆ†éš”è¿æ¥çš„å­—ç¬¦ä¸²
    """
    errors = []

    if lang == 'en':
        stopwords_en = set(nltk.corpus.stopwords.words('english'))
    elif lang == 'zh':
        stopwords_zh = set(nltk.corpus.stopwords.words('chinese'))
    else: 
        raise ValueError("Unsupported language. Use 'en' or 'zh'.")
            
    # tokens = nltk.word_tokenize(processed) if lang == 'en' else jieba.lcut(processed)
    tokens = processed.split()

    if lang == 'en' and any(token in stopwords_en for token in tokens):
        errors.append("è‹±æ–‡åœç”¨è¯æœªç§»é™¤")
    if lang == 'zh' and any(token in stopwords_zh for token in tokens):
        errors.append("ä¸­æ–‡åœç”¨è¯æœªç§»é™¤")
        
    return errors

def test_preprocessed_text(processed, lang)->list:
    """éªŒè¯é¢„å¤„ç†ç»“æœ"""
    errors = []
    # æ£€æŸ¥HTMLæ ‡ç­¾æ˜¯å¦ç§»é™¤ï¼ˆå‡è®¾åŸå§‹æ–‡æœ¬å«HTMLæ ‡ç­¾ï¼Œæ­¤å¤„ç®€åŒ–ä¸ºæ£€æŸ¥ç‰¹æ®Šæ ‡ç­¾å­—ç¬¦ï¼‰
    if '<' in processed or '>' in processed:
        errors.append("HTMLæ ‡ç­¾æœªå®Œå…¨ç§»é™¤")
    
    # æ£€æŸ¥å°å†™è½¬æ¢
    if any(c.isupper() for c in processed) and lang == 'en':
        errors.append("è‹±æ–‡æ–‡æœ¬æœªè½¬ä¸ºå°å†™")
    
    # æ£€æŸ¥è‹±æ–‡ç¼©å†™æ‰©å±•ï¼ˆç¤ºä¾‹ï¼šdon't â†’ do notï¼‰
    if lang == 'en':
        err_test_english_contractions = test_english_contractions(processed) 
        if len(err_test_english_contractions)>0: # not empty
            errors.extend(err_test_english_contractions)
    
    # æ£€æŸ¥åœç”¨è¯ç§»é™¤ï¼ˆè‹±æ–‡ç¤ºä¾‹ï¼š'the'ï¼›ä¸­æ–‡ç¤ºä¾‹ï¼š'çš„'ï¼‰
    err_test_stopwords = test_stopwords(processed, lang)
    if len(err_test_stopwords)>0:
        errors.extend(err_test_stopwords)

    # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦ç§»é™¤ï¼ˆç¤ºä¾‹ï¼š'!' '?' ç­‰ï¼‰
    if any(c in '!@#$%^&*()_+' for c in processed):
        errors.append("ç‰¹æ®Šå­—ç¬¦æœªç§»é™¤")
    
    return errors

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="å¤„ç†ä¹¦ç±åˆ—è¡¨å¹¶ç”Ÿæˆç»“æœï¼ˆé…ç½®æ–‡ä»¶ä¸ºJSONæ ¼å¼ï¼‰")
    parser.add_argument('config_file', help="ä¹¦ç±åˆ—è¡¨é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ booklist.jsonï¼‰")
    args = parser.parse_args()

    # è¯»å–JSONé…ç½®
    try:
        with open(args.config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except Exception as e:
        print(f"è¯»å–JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
        return
    
    # è§£æé…ç½®æ•°æ®
    book_dict = dict(config.get('booklist', []))
    params_dict = dict(config.get('preprocessing_params', []))
    
    print("# é¢„å¤„ç†ç»“æœæŠ¥å‘Š") 
    
    for book_name, file_path in tqdm(book_dict.items(), desc="å¤„ç†ä¹¦ç±"):
        print(f"\n## å¤„ç†ä¹¦ç±ï¼šã€Š{book_name}ã€‹")
        print(f"\n### ä¹¦ç±è·¯å¾„å’Œå¤„ç†å‚æ•°")
        print(f"* æºæ–‡ä»¶è·¯å¾„: {file_path}")

        # è·å–é¢„å¤„ç†å‚æ•°
        if book_name not in params_dict:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° {book_name} çš„é¢„å¤„ç†å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            params = {
                "html_stripping": True,
                "contraction_expansion": True,
                "accented_char_removal": True,
                "text_lower_case": True,
                "text_lemmatization": True,
                "special_char_removal": True,
                "stopword_removal": True,
                "remove_digits": False,
                "zh_simplification": True,
                "isDebug": False
            }
        else:
            params = params_dict[book_name]
            print("* é¢„å¤„ç†å‚æ•°:")
            for key, value in params.items():
                print(f"  * {key}: {value}")
            print()        

        # 1. è¯»å–æ•°æ®å†…å®¹
        original_doc = fetch_gutenberg_text(file_path)
        if not original_doc:
            print(f"é”™è¯¯ï¼š æ— æ³•è¯»å–ä¹¦ç±å†…å®¹ï¼Œè·³è¿‡å¤„ç†")
            continue    
                
        print("###1. æˆåŠŸè·å–æ–‡æœ¬")
        print(f"* é•¿åº¦{len(original_doc)}å­—ç¬¦")
        
        # 2. æ‰§è¡Œé¢„å¤„ç†
        processed_doc, lang = normalize_doc(
            original_doc, **params     # **params å°†å­—å…¸è§£åŒ…ä¸ºå…³é”®å­—å‚æ•°         
        )

        print("###2. é¢„å¤„ç†å®Œæˆ")
        print(f"* è¯­è¨€ï¼š{lang}ï¼Œå¤„ç†åé•¿åº¦ï¼š{len(processed_doc)}å­—ç¬¦")
           
        if lang == 'unknown' or not processed_doc:
            print("é”™è¯¯: è·³è¿‡æ£€æŸ¥ï¼ˆæœªçŸ¥è¯­è¨€æˆ–ç©ºæ–‡æœ¬ï¼‰")
            continue

        # 3. ç»“æœè¾“å‡ºå­˜å‚¨
        print("###3. ç»“æœè¾“å‡ºå­˜å‚¨")
        save_processed_text(file_path, processed_doc)  # è°ƒç”¨ä¿å­˜å‡½æ•°

        # 4. è¾“å‡ºå‰10é«˜é¢‘è¯åŠå‰20é•¿çš„å•è¯
        n, k = 10, 20
        top_n, longest_k = get_statistics(processed_doc, n=n, k=k, lang=lang)
        print(f"###4. è¾“å‡ºæ–‡æœ¬å‰{n}é«˜é¢‘è¯å’Œå‰{k}é•¿çš„å•è¯:")
        print(f"* å‰{n}é«˜é¢‘è¯ç»Ÿè®¡")
        print("| è¯è¯­ | å‡ºç°é¢‘ç‡ |")
        print("|------|----------|")
        for word, freq in top_n:
            print(f"| {word} | {freq} |")
        print()
        
        print(f"* å‰{k}é•¿è¯ç»Ÿè®¡")
        print("| è¯è¯­ | é•¿åº¦ |")
        print("|------|----------|")
        for word, len_w in longest_k:
            print(f"| {word} | {len_w} |")
        print()

        print(f"## æ£€æŸ¥é¢„å¤„ç†ç»“æœï¼šã€Š{book_name}ã€‹")            
        
        # è·å–åŸå§‹æ–‡æœ¬å’Œå¤„ç†åæ–‡æœ¬
        errors = test_preprocessed_text(processed_doc, lang) if processed_doc else []
        if not errors:
            print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡")
        else:
            print("âŒ é”™è¯¯ï¼š")
            for e in errors:
                print(f"- {e}")

if __name__ == "__main__":
    # æ‰§è¡Œä¸»å‡½æ•°
    main()