# -*- coding: utf-8 -*-
# @Author: Haiqin Yang
# @Date:   2025-10-07 16:12:11
# @Last Modified by:   Haiqin Yang
# @Last Modified time: 2025-10-07 17:03:41
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')

from nltk.stem import PorterStemmer

from nltk.corpus import wordnet, stopwords

from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
import re
import unicodedata
import contractions
from bs4 import BeautifulSoup
import jieba  # ä¸­æ–‡åˆ†è¯
from opencc import OpenCC  # ä¸­æ–‡ç¹ä½“è½¬ç®€ä½“
from util import detect_language # å¯¼å…¥éœ€è¦çš„å‡½æ•°

# åˆå§‹åŒ–å·¥å…·
wnl = WordNetLemmatizer()
cc_zh = OpenCC('t2s')  # ä¸­æ–‡ç¹ä½“è½¬ç®€ä½“

def strip_html_tags(text):
    """
    ç§»é™¤æ–‡æœ¬ä¸­çš„HTMLæ ‡ç­¾å¹¶æå–çº¯æ–‡æœ¬å†…å®¹ï¼ŒåŒæ—¶æ ‡å‡†åŒ–æ¢è¡Œç¬¦ã€‚

    åŠŸèƒ½è¯´æ˜ï¼š
        1. ä½¿ç”¨BeautifulSoupè§£æHTMLæ–‡æœ¬ï¼Œç§»é™¤`<iframe>`å’Œ`<script>`æ ‡ç­¾ï¼ˆé€šå¸¸åŒ…å«éæ–‡æœ¬å†…å®¹ï¼‰
        2. æå–HTMLä¸­çš„çº¯æ–‡æœ¬å†…å®¹
        3. å°†å„ç§æ¢è¡Œç¬¦ï¼ˆ\rã€\nã€\r\nï¼‰ç»Ÿä¸€æ›¿æ¢ä¸ºå•ä¸ª\nï¼Œç¡®ä¿æ¢è¡Œæ ¼å¼ä¸€è‡´

    å‚æ•°ï¼š
        text (str)ï¼šåŒ…å«HTMLæ ‡ç­¾çš„åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²

    è¿”å›ï¼š
        strï¼šç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾åçš„çº¯æ–‡æœ¬ï¼Œæ¢è¡Œç¬¦å·²æ ‡å‡†åŒ–ä¸º\n

    ä¾èµ–ï¼š
        éœ€è¦å®‰è£…BeautifulSoupåº“ï¼ˆpip install beautifulsoup4ï¼‰

    ç¤ºä¾‹ï¼š
        >>> raw_html = "<p>Hello<br>World!</p><script>alert('test')</script>"
        >>> strip_html_tags(raw_html)
        'Hello\nWorld!'
    """
    pass
    
def remove_accented_chars(text):
    """
    ç§»é™¤æ–‡æœ¬ä¸­çš„é‡éŸ³å­—ç¬¦ï¼ˆå¦‚Ã©ã€Ã±ã€Ã¼ç­‰ï¼‰ï¼Œå°†å…¶è½¬æ¢ä¸ºæ— é‡éŸ³çš„åŸºç¡€å­—ç¬¦ã€‚
    
    å¤„ç†é€»è¾‘ï¼š
        1. ä½¿ç”¨`unicodedata.normalize('NFKD', text)`å¯¹æ–‡æœ¬è¿›è¡ŒUnicodeè§„èŒƒåŒ–ï¼š
           - NFKDï¼ˆCompatibility Decomposition, Canonical Compositionï¼‰ä¼šå°†é‡éŸ³å­—ç¬¦åˆ†è§£ä¸º
             åŸºç¡€å­—ç¬¦ + é‡éŸ³ç¬¦å·ï¼ˆä¾‹å¦‚ï¼š'Ã©' åˆ†è§£ä¸º 'e' + é‡éŸ³ç¬¦å·ï¼‰ã€‚
        2. é€šè¿‡`.encode('ascii', 'ignore')`å°†è§„èŒƒåŒ–åçš„æ–‡æœ¬ç¼–ç ä¸ºASCIIï¼š
           - ASCIIç¼–ç ä¸æ”¯æŒé‡éŸ³ç¬¦å·ï¼Œ`ignore`å‚æ•°ä¼šå¿½ç•¥æ— æ³•ç¼–ç çš„é‡éŸ³ç¬¦å·éƒ¨åˆ†ï¼Œä»…ä¿ç•™åŸºç¡€å­—ç¬¦ã€‚
        3. å†é€šè¿‡`.decode('utf-8', 'ignore')`å°†å­—èŠ‚æµè§£ç å›UTF-8å­—ç¬¦ä¸²ï¼Œå¾—åˆ°æ— é‡éŸ³çš„ç»“æœã€‚
    
    å‚æ•°ï¼š
        text (str)ï¼šåŒ…å«é‡éŸ³å­—ç¬¦çš„åŸå§‹æ–‡æœ¬
    
    è¿”å›ï¼š
        strï¼šç§»é™¤é‡éŸ³åçš„æ–‡æœ¬ï¼ˆä»…åŒ…å«ASCIIå¯è¡¨ç¤ºçš„å­—ç¬¦ï¼‰
    
    ç¤ºä¾‹ï¼š
        >>> remove_accented_chars("cafÃ© clichÃ© naÃ¯ve")
        'cafe cliche naive'
        >>> remove_accented_chars("Ã Ã¨Ã¬Ã²Ã¹ ÃÃ‰ÃÃ“Ãš Ã± Ã§")
        'aeiou AEIOU n c'
    """
    pass
    
def pos_tag_wordnet(tagged_tokens):
    """
    å°†NLTKè¯æ€§æ ‡æ³¨ç»“æœè½¬æ¢ä¸ºWordNetè¯å½¢è¿˜åŸå™¨ï¼ˆLemmatizerï¼‰å…¼å®¹çš„è¯æ€§æ ‡ç­¾ã€‚
    
    èƒŒæ™¯ï¼š
        NLTKçš„`pos_tag`å‡½æ•°è¿”å›çš„è¯æ€§æ ‡ç­¾éµå¾ªPenn Treebankæ ¼å¼ï¼ˆå¦‚å½¢å®¹è¯ä¸º'JJ'ã€åŠ¨è¯ä¸º'VB'ç­‰ï¼‰ï¼Œ
        è€ŒWordNetçš„`lemmatize`æ–¹æ³•éœ€è¦ç‰¹å®šçš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚å½¢å®¹è¯ä¸º`wordnet.ADJ`ã€åŠ¨è¯ä¸º`wordnet.VERB`ç­‰ï¼‰ï¼Œ
        å› æ­¤éœ€è¦é€šè¿‡é¦–å­—æ¯æ˜ å°„å®ç°æ ¼å¼è½¬æ¢ã€‚
    
    å‚æ•°ï¼š
        tagged_tokens (list)ï¼šç”±`nltk.pos_tag`è¿”å›çš„è¯æ€§æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„`(word, tag)`ï¼Œ
                             å…¶ä¸­`word`æ˜¯è¯è¯­ï¼Œ`tag`æ˜¯Penn Treebankæ ¼å¼çš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚'JJ'ã€'VB'ï¼‰ã€‚
    
    è¿”å›ï¼š
        listï¼šè½¬æ¢åçš„è¯æ€§æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„`(word, wordnet_tag)`ï¼Œ
              å…¶ä¸­`wordnet_tag`æ˜¯WordNetå…¼å®¹çš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚`wordnet.ADJ`ã€`wordnet.VERB`ï¼‰ã€‚
              è‹¥æ ‡ç­¾æ— æ³•åŒ¹é…ï¼Œé»˜è®¤æ˜ å°„ä¸ºåè¯ï¼ˆ`wordnet.NOUN`ï¼‰ã€‚
    
    æ˜ å°„è§„åˆ™ï¼š
        - 'j'ï¼ˆå½¢å®¹è¯ï¼Œå¦‚Pennæ ‡ç­¾'JJ'ï¼‰â†’ `wordnet.ADJ`
        - 'v'ï¼ˆåŠ¨è¯ï¼Œå¦‚Pennæ ‡ç­¾'VB'ï¼‰â†’ `wordnet.VERB`
        - 'n'ï¼ˆåè¯ï¼Œå¦‚Pennæ ‡ç­¾'NN'ï¼‰â†’ `wordnet.NOUN`
        - 'r'ï¼ˆå‰¯è¯ï¼Œå¦‚Pennæ ‡ç­¾'RB'ï¼‰â†’ `wordnet.ADV`
    
    ç¤ºä¾‹ï¼š
        >>> from nltk import pos_tag, word_tokenize
        >>> tagged = pos_tag(word_tokenize("running fast"))  # [('running', 'VBG'), ('fast', 'RB')]
        >>> pos_tag_wordnet(tagged)
        [('running', wordnet.VERB), ('fast', wordnet.ADV)]
    """
    pass


def lemmatize_text(text):
    """
    å¯¹æ–‡æœ¬è¿›è¡Œè¯å½¢è¿˜åŸï¼ˆLemmatizationï¼‰ï¼Œå°†è¯è¯­è¿˜åŸä¸ºå…¶åŸºæœ¬å½¢å¼ï¼ˆå¦‚åŠ¨è¯ç¬¬ä¸‰äººç§°â†’åŸå½¢ã€åè¯å¤æ•°â†’å•æ•°ï¼‰ã€‚
    
    å¤„ç†æµç¨‹ï¼š
        1. åˆ†è¯ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºç‹¬ç«‹è¯è¯­ï¼ˆä½¿ç”¨NLTKçš„`word_tokenize`ï¼‰ã€‚
        2. è¯æ€§æ ‡æ³¨ï¼šä¸ºæ¯ä¸ªè¯è¯­æ·»åŠ Penn Treebankæ ¼å¼çš„è¯æ€§æ ‡ç­¾ï¼ˆä½¿ç”¨NLTKçš„`pos_tag`ï¼‰ã€‚
        3. æ ‡ç­¾è½¬æ¢ï¼šå°†è¯æ€§æ ‡ç­¾è½¬æ¢ä¸ºWordNetå…¼å®¹æ ¼å¼ï¼ˆè°ƒç”¨`pos_tag_wordnet`ï¼‰ã€‚
        4. è¯å½¢è¿˜åŸï¼šä½¿ç”¨WordNetè¯å½¢è¿˜åŸå™¨ï¼Œæ ¹æ®è½¬æ¢åçš„è¯æ€§æ ‡ç­¾å¯¹æ¯ä¸ªè¯è¯­è¿›è¡Œè¿˜åŸã€‚
        5. æ‹¼æ¥ï¼šå°†è¿˜åŸåçš„è¯è¯­é‡æ–°æ‹¼æ¥ä¸ºæ–‡æœ¬ã€‚
    
    å‚æ•°ï¼š
        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ï¼ˆè‹±æ–‡ï¼‰ã€‚
    
    è¿”å›ï¼š
        strï¼šç»è¿‡è¯å½¢è¿˜åŸåçš„æ–‡æœ¬ï¼Œè¯è¯­å‡ä¸ºåŸºæœ¬å½¢å¼ã€‚
    
    ä¾èµ–ï¼š
        - éœ€è¦åŠ è½½NLTKçš„`punkt`åˆ†è¯æ¨¡å‹å’Œ`averaged_perceptron_tagger`è¯æ€§æ ‡æ³¨æ¨¡å‹ï¼ˆå¯é€šè¿‡`nltk.download()`ä¸‹è½½ï¼‰ã€‚
        - éœ€è¦åˆå§‹åŒ–WordNetè¯å½¢è¿˜åŸå™¨ï¼ˆå¦‚`wnl = WordNetLemmatizer()`ï¼‰ã€‚
    
    ç¤ºä¾‹ï¼š
        >>> wnl = WordNetLemmatizer()  # å‡è®¾å·²åˆå§‹åŒ–
        >>> lemmatize_text("Cats are running quickly")
        'cat be run quick'
    """
    pass       

def remove_special_characters(text, lang='en', remove_digits=False):
    """
    ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ã€ç¬¦å·ç­‰ï¼‰ï¼Œå¯é€‰æ‹©æ˜¯å¦ä¿ç•™æ•°å­—ã€‚
    
    å¤„ç†é€»è¾‘ï¼š
        1. æ ¹æ®`remove_digits`å‚æ•°æ„å»ºæ­£åˆ™åŒ¹é…æ¨¡å¼:
           - å½“`remove_digits=False`ï¼ˆé»˜è®¤ï¼‰:
             - lang='en': ä¿ç•™å­—æ¯(a-zA-Z)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚
             - lang='zh': ä¿ç•™ä¸­æ–‡(ä¸€-é¾¥)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚
           - å½“`remove_digits=True`:
             - lang='en': ä¿ç•™å­—æ¯(a-zA-Z)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚
             - lang='zh': ä¿ç•™ä¸­æ–‡(ä¸€-é¾¥)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚
        2. ä½¿ç”¨`re.sub`å°†åŒ¹é…åˆ°çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå®ç°ç§»é™¤æ•ˆæœã€‚
        3. é¢å¤–å¤„ç†ï¼šç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦ï¼Œç¡®ä¿æ–‡æœ¬æ•´æ´ã€‚
    
    å‚æ•°ï¼š
        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ã€‚
        remove_digits (bool)ï¼šæ˜¯å¦ç§»é™¤æ•°å­—ï¼Œé»˜è®¤Falseï¼ˆä¿ç•™æ•°å­—ï¼‰ã€‚
        remove_special_characters (bool): æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤Falseï¼ˆä¿ç•™ç‰¹æ®Šå­—ç¬¦ï¼‰
    
    è¿”å›ï¼š
        strï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦åçš„æ–‡æœ¬ï¼ˆä»…ä¿ç•™æŒ‡å®šå…è®¸çš„å­—ç¬¦ï¼‰ã€‚
    
    ç¤ºä¾‹ï¼š
        >>> remove_special_characters("Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚", lang='en')
        'Hello  123'
        
        >>> remove_special_characters("Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚", lang='zh')
        'ä¸–ç•Œ 123'
        
        >>> remove_special_characters("Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚", lang='en', remove_digits=True)
        'Hello '
        
        >>> remove_special_characters("Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚", lang='zh', remove_digits=True)
        'ä¸–ç•Œ'    
    """
    pass

def remove_stopwords(text, is_lower_case=False, stopwords=None, lang='en'):
    """
    ç§»é™¤æ–‡æœ¬ä¸­çš„åœç”¨è¯ï¼ˆå¦‚è‹±æ–‡çš„"the"ã€"is"ï¼Œä¸­æ–‡çš„"çš„"ã€"äº†"ç­‰æ— å®é™…è¯­ä¹‰çš„é«˜é¢‘è¯ï¼‰ï¼Œæ”¯æŒä¸­è‹±æ–‡ã€‚
    
    å¤„ç†æµç¨‹ï¼š
        1. åŠ è½½åœç”¨è¯è¡¨ï¼šè‹¥æœªæä¾›è‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼ˆstopwordsï¼‰ï¼Œåˆ™æ ¹æ®è¯­è¨€ï¼ˆlangï¼‰åŠ è½½é»˜è®¤åœç”¨è¯è¡¨ï¼š
           - è‹±æ–‡ï¼šä½¿ç”¨NLTKçš„è‹±æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.corpus.stopwords.words('english')ï¼‰ã€‚
           - ä¸­æ–‡ï¼šä½¿ç”¨NLTKçš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.corpus.stopwords.words('chinese')ï¼‰ã€‚
        2. åˆ†è¯ï¼šæ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯å·¥å…·ï¼š
           - è‹±æ–‡ï¼šä½¿ç”¨NLTKçš„`word_tokenize`è¿›è¡Œåˆ†è¯ã€‚
           - ä¸­æ–‡ï¼šä½¿ç”¨Jiebaçš„`lcut`ï¼ˆç²¾ç¡®æ¨¡å¼ï¼‰è¿›è¡Œåˆ†è¯ã€‚
        3. è¿‡æ»¤åœç”¨è¯ï¼šæ ¹æ®`is_lower_case`åˆ¤æ–­æ˜¯å¦éœ€è¦å°†è¯è¯­å°å†™åå†åŒ¹é…åœç”¨è¯è¡¨ï¼Œä¿ç•™éåœç”¨è¯ã€‚
        4. æ‹¼æ¥ï¼šå°†è¿‡æ»¤åçš„è¯è¯­é‡æ–°æ‹¼æ¥ä¸ºæ–‡æœ¬ã€‚
    
    å‚æ•°ï¼š
        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ã€‚
        is_lower_case (bool)ï¼šæ–‡æœ¬æ˜¯å¦å·²è½¬ä¸ºå°å†™ï¼Œé»˜è®¤Falseï¼ˆéœ€å°†è¯è¯­å°å†™åå†åŒ¹é…åœç”¨è¯ï¼‰ã€‚
        stopwords (list, optional)ï¼šè‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼Œè‹¥ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è¡¨ã€‚
        lang (str)ï¼šè¯­è¨€ç±»å‹ï¼Œ'en'ï¼ˆè‹±æ–‡ï¼‰æˆ–'zh'ï¼ˆä¸­æ–‡ï¼‰ï¼Œé»˜è®¤'en'ã€‚
    
    è¿”å›ï¼š
        strï¼šåˆ‡è¯å¹¶ç§»é™¤åœç”¨è¯åç”¨ç©ºæ ¼ç›¸è¿çš„æ–‡æœ¬ã€‚
    
    ä¾èµ–ï¼š
        - è‹±æ–‡ï¼šéœ€åŠ è½½NLTKçš„`punkt`åˆ†è¯æ¨¡å‹ï¼ˆnltk.download('punkt')ï¼‰å’Œè‹±æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.download('stopwords')ï¼‰ã€‚
        - ä¸­æ–‡ï¼šéœ€å®‰è£…Jiebaï¼ˆpip install jiebaï¼‰å’ŒåŠ è½½NLTKçš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.download('stopwords')ï¼‰ã€‚
    
    ç¤ºä¾‹ï¼š
        >>> # è‹±æ–‡ç¤ºä¾‹
        >>> remove_stopwords("The quick brown fox jumps over the lazy dog", lang='en')
        'quick brown fox jumps lazy dog'
        >>> # ä¸­æ–‡ç¤ºä¾‹
        >>> remove_stopwords("è¿™åªæ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†é‚£åªæ‡’ç‹—", lang='zh')
        'åª æ•æ· æ£•è‰² ç‹ç‹¸ è·³è¿‡ åª æ‡’ ç‹—'
    """
    pass    
 
def normalize_doc(doc, 
                     html_stripping=True, 
                     contraction_expansion=True,
                     accented_char_removal=True, 
                     text_lower_case=True,
                     text_lemmatization=True, 
                     special_char_removal=True,
                     stopword_removal=True, 
                     remove_digits=False, # Default: keep digits
                     zh_simplification=True,
                     isDebug=False):
    """
    è§„èŒƒåŒ–æ–‡æœ¬è¯­æ–™åº“ï¼Œæ”¯æŒå¤šç§é¢„å¤„ç†æ“ä½œï¼ŒåŒ…æ‹¬HTMLæ ‡ç­¾ç§»é™¤ã€ç¼©å†™æ‰©å±•ã€é‡éŸ³å­—ç¬¦ç§»é™¤ã€
    å°å†™è½¬æ¢ã€è¯å½¢è¿˜åŸã€ç‰¹æ®Šå­—ç¬¦ç§»é™¤ã€åœç”¨è¯ç§»é™¤ç­‰ï¼Œé€‚ç”¨äºä¸­è‹±æ–‡æ–‡æœ¬ã€‚
    å‚æ•°ï¼š
        doc (str): è¾“å…¥æ–‡æ¡£ã€‚
        html_stripping (bool): æ˜¯å¦ç§»é™¤HTMLæ ‡ç­¾ï¼Œé»˜è®¤Trueã€‚
        contraction_expansion (bool): æ˜¯å¦æ‰©å±•ç¼©å†™ï¼Œé»˜è®¤Trueã€‚
        accented_char_removal (bool): æ˜¯å¦ç§»é™¤é‡éŸ³å­—ç¬¦ï¼Œé»˜è®¤Trueã€‚
        text_lower_case (bool): æ˜¯å¦å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œé»˜è®¤Trueã€‚
        text_lemmatization (bool): æ˜¯å¦è¿›è¡Œè¯å½¢è¿˜åŸï¼Œé»˜è®¤Trueã€‚
        special_char_removal (bool): æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤Trueã€‚
        stopword_removal (bool): æ˜¯å¦ç§»é™¤åœç”¨è¯ï¼Œé»˜è®¤Trueã€‚
        remove_digits (bool): æ˜¯å¦ç§»é™¤æ•°å­—ï¼Œé»˜è®¤Falseã€‚
        zh_simplification (bool): æ˜¯å¦å°†ä¸­æ–‡ç¹ä½“è½¬æ¢ä¸ºç®€ä½“ï¼Œé»˜è®¤Trueã€‚
        isDebug (bool): æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œé»˜è®¤Falseã€‚
    è¿”å›ï¼š
        å…ƒç»„ `(doc, lang)`ï¼Œå…¶ä¸­ï¼š
            - doc (str)ï¼šé¢„å¤„ç†åçš„æ–‡æœ¬(åˆ‡è¯åç”¨ç©ºæ ¼ç›¸è¿)
            - lang (str)ï¼šæ–‡æœ¬è¯­è¨€ï¼ˆ'en'/'zh'/'unknown'ï¼‰    
    """                              
    pass
